{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Naive Bayes Classifier is a collection of classifiers that apply Bayes theorem between features in our dataset. We are going to look at this type of classifier in a lot of detail throughout this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code used to preprocess our dataset. \n",
    "# Each step is explained in detail in the 'Data Pre-processing' notebook.\n",
    "#It is important to have this code in the notebook so we use the array with the NB Classifier\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('news.csv')\n",
    "df['news'] = df['title'] + ' ' + df['text']\n",
    "convert_to_binary = {'REAL':1,'FAKE':0}\n",
    "df['label'] = df['label'].map(convert_to_binary)\n",
    "df = df.drop([df.columns[0],df.columns[1],df.columns[2]],axis=1)\n",
    "df = df.reindex(columns=['news','label'])\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['the','it','in'])\n",
    "WNL = WordNetLemmatizer()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    filtered_article = ''\n",
    "    article = row['news']\n",
    "    article = re.sub(r'[^\\w\\s]', '', article)\n",
    "    words = [word.lower() for word in nltk.word_tokenize(article)]\n",
    "    words = [word for word in words if not word in stop_words]\n",
    "    words_lemmatized = []\n",
    "    for word in words:\n",
    "        if word == 'us':\n",
    "            words_lemmatized.append(word)\n",
    "        else:\n",
    "            words_lemmatized.append(WNL.lemmatize(word))\n",
    "    filtered_article = \" \".join([word for word in words_lemmatized])\n",
    "    df.loc[index, 'news'] = filtered_article\n",
    "    \n",
    "df.head()\n",
    "\n",
    "\n",
    "# We need the Vectorization\n",
    "\n",
    "df_input = df['news']\n",
    "df_output = df['label']\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tf_idf_matrix = vectorizer.fit_transform(df_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of Naive Bayes is for classification of data. We will experiment with this type of Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial Naive Bayes is a type of Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(tf_idf_matrix, df_output, test_size = 0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have split our data into training data and test data. We can now input our training data into the Gaussian Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8011572856391374\n",
      "CPU times: user 23.6 ms, sys: 1.77 ms, total: 25.3 ms\n",
      "Wall time: 28.7 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "MNB = MultinomialNB()\n",
    "MNB.fit (x_train,y_train)\n",
    "Accuracy = MNB.score(x_test, y_test)\n",
    "print(Accuracy)\n",
    "%time MNB.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8125,\n",
       " 0.8267716535433071,\n",
       " 0.8272251308900523,\n",
       " 0.8346456692913385,\n",
       " 0.8264984227129337,\n",
       " 0.8320209973753281,\n",
       " 0.831081081081081,\n",
       " 0.8264299802761341,\n",
       " 0.8283712784588442,\n",
       " 0.832807570977918,\n",
       " 0.8321377331420373,\n",
       " 0.8331143232588699,\n",
       " 0.8300970873786407,\n",
       " 0.8387824126268321,\n",
       " 0.8391167192429022,\n",
       " 0.8274161735700197,\n",
       " 0.819870009285051,\n",
       " 0.8220858895705522,\n",
       " 0.8264119601328903,\n",
       " 0.8232044198895028,\n",
       " 0.8204357625845229,\n",
       " 0.8199426111908178,\n",
       " 0.8237311385459534,\n",
       " 0.8244575936883629,\n",
       " 0.8200757575757576,\n",
       " 0.8125,\n",
       " 0.8158971361776739,\n",
       " 0.8117249154453213,\n",
       " 0.8041349292709467,\n",
       " 0.8011572856391374,\n",
       " 0.8014256619144603,\n",
       " 0.7958579881656804,\n",
       " 0.7924438067910091,\n",
       " 0.7938718662952646,\n",
       " 0.7953110910730388,\n",
       " 0.7860587461639632,\n",
       " 0.7794368600682594,\n",
       " 0.7836378737541528,\n",
       " 0.7842978551193849,\n",
       " 0.7880820836621941,\n",
       " 0.7825250192455735,\n",
       " 0.7812852311161218,\n",
       " 0.7790825688073395,\n",
       " 0.7786944045911047,\n",
       " 0.7730620834794809,\n",
       " 0.7725557461406518,\n",
       " 0.7696440564137005,\n",
       " 0.7737586320289378,\n",
       " 0.7729468599033816]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = np.arange(0.01,0.5,0.01)\n",
    "accuracy = []\n",
    "for i in size:\n",
    "    x_train, x_test, y_train, y_test = train_test_split(tf_idf_matrix, df_output, test_size = i, random_state = 42)\n",
    "    MNB.fit(x_train, y_train)\n",
    "    a = MNB.score(x_test,y_test)\n",
    "    accuracy.append(a)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy of Multinomial Naive Bayes Classifier')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(size, accuracy)\n",
    "plt.xlabel('Test Size')\n",
    "plt.ylabel('Accuracy of Multinomial Naive Bayes Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15000000000000002"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size[np.argmax(accuracy)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to now finetune the parameter fit_prior to see if this has an effect on the accuracy or not. There are other parameters involved with Multinomial Naive Bayes however the key parameter is the fit_prior parameter whose default is True. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7761674718196457\n",
      "CPU times: user 14.7 ms, sys: 967 Âµs, total: 15.7 ms\n",
      "Wall time: 15.6 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNB_FALSE = MultinomialNB(fit_prior=False)\n",
    "MNB_FALSE.fit (x_train,y_train)\n",
    "Accuracy = MNB_FALSE.score(x_test, y_test)\n",
    "print(Accuracy)\n",
    "%time MNB_FALSE.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A way in which we can interpret how well our classifiers work with our chosen data set is by creating a confusion matrix. Also known as an error matrix, it is a great way in enabling us to visualise how a supervised learning technique performs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 871  690]\n",
      " [  15 1529]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.56      0.71      1561\n",
      "           1       0.69      0.99      0.81      1544\n",
      "\n",
      "    accuracy                           0.77      3105\n",
      "   macro avg       0.84      0.77      0.76      3105\n",
      "weighted avg       0.84      0.77      0.76      3105\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test,MNB.predict(x_test)))\n",
    "print(classification_report(y_test,MNB.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on Unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to see how Naive Bayes classifier works for unseen data. It will be interesting to see how effective it is on a completely different data set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['the','it','in'])\n",
    "WNL = WordNetLemmatizer()\n",
    "    \n",
    "\n",
    "def article_preprocessor (article):\n",
    "    filtered_article = ''\n",
    "    article = re.sub(r'[^\\w\\s]', '', article)\n",
    "    words = [word.lower() for word in nltk.word_tokenize(article)]\n",
    "    words = [word for word in words if not word in stop_words]\n",
    "    words_lemmatized = []\n",
    "    for word in words:\n",
    "        if word == 'us':\n",
    "            words_lemmatized.append(word)\n",
    "        else:\n",
    "            words_lemmatized.append(WNL.lemmatize(word))\n",
    "    filtered_article = \" \".join([word for word in words_lemmatized])\n",
    "    return filtered_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultinomialNB_classifier (list_of_articles):\n",
    "    \n",
    "    # Pre-process the articles\n",
    "    articles_pp = [article_preprocessor(article) for article in list_of_articles]\n",
    "    new_input = df_input.append(pd.Series(articles_pp))\n",
    "    tf_idf_matrix = vectorizer.fit_transform(new_input)\n",
    "    orig_data_matrix = tf_idf_matrix[:len(df_input)]\n",
    "    new_data_matrix = tf_idf_matrix[len(df_input):]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(orig_data_matrix, df_output, random_state=42)\n",
    "    MNB = MultinomialNB()\n",
    "    MNB.fit(x_train, y_train)\n",
    "    accuracy = MNB.score(x_test,y_test)\n",
    "    print('Multinomial Naive Bayes model accuracy: ' + str(accuracy))\n",
    "    # The model can now classify the new data\n",
    "    predictions = MNB.predict(new_data_matrix)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The top news story on the BBC\n",
    "bbc_news_article = '''The furlough scheme will be extended until the end of September by the chancellor in the Budget later.\n",
    "Rishi Sunak said the scheme - which pays 80% of employees' wages for the hours they cannot work in the pandemic - would help millions through \"the challenging months ahead\".\n",
    "Some 600,000 more self-employed people will also be eligible for government help as access to grants is widened.\n",
    "But Labour said the support schemes should have been extended \"months ago\".\n",
    "Mr Sunak will outline a three-point plan to support people through the coming months, rebuild the economy and \"fix\" the public finances in the wake of the pandemic when he delivers his statement to the Commons at about 12:30 GMT.\n",
    "But he has warned of tough economic times ahead and there are reports that he plans to raise some taxes.'''\n",
    "\n",
    "# Here's a fake news article from the New York Mag\n",
    "fake_article = '''Twelve days out from judgment day in an election in which he continues to trail badly, President Trump continues to hammer home an issue that will surely resonate with that small slice of still-undecided voters: his supposedly unfair treatment at the hands of CBSâs Lesley Stahl. After two days of promising to release unedited footage of an as-yet-unaired 60 Minutes interview, during which he walked out prematurely because he was upset with Stahlâs line of questioning, the president finally followed through on Thursday. Throughout the interview, Stahl presses Trump on issues from health care (the president says he hopes the Supreme Court strikes down Obamacare, a politically toxic position) to his derogatory comments about Anthony Fauci (Trump claims he was misinterpreted) to his false claims that the Obama campaign spied on him. The tone is of an adversarial back-and-forth, well within normal journalistic bounds. Nevertheless, Trump continuously claims that Joe Biden hasnât been given similar treatment by CBS and cuts the proceedings short.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes model accuracy: 0.8200757575757576\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = [bbc_news_article,fake_article]\n",
    "MultinomialNB_classifier(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it predicted that both the articles were true, however the second news article was in fact false!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
