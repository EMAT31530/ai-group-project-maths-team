{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "Here we shall implement the logistic regression model to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>smell hillary fear daniel greenfield shillman ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>watch exact moment paul ryan committed politic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>kerry go paris gesture sympathy us secretary s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>bernie supporter twitter erupt anger dnc tried...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>battle new york primary matter primary day new...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                news  label\n",
       "0  smell hillary fear daniel greenfield shillman ...      0\n",
       "1  watch exact moment paul ryan committed politic...      0\n",
       "2  kerry go paris gesture sympathy us secretary s...      1\n",
       "3  bernie supporter twitter erupt anger dnc tried...      0\n",
       "4  battle new york primary matter primary day new...      1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the code used to preprocess our dataset. \n",
    "# Each step is explained in detail in the 'Data Pre-processing' notebook.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('news.csv')\n",
    "df['news'] = df['title'] + ' ' + df['text']\n",
    "convert_to_binary = {'REAL':1,'FAKE':0}\n",
    "df['label'] = df['label'].map(convert_to_binary)\n",
    "df = df.drop([df.columns[0],df.columns[1],df.columns[2]],axis=1)\n",
    "df = df.reindex(columns=['news','label'])\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['the','it','in'])\n",
    "WNL = WordNetLemmatizer()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    filtered_article = ''\n",
    "    article = row['news']\n",
    "    article = re.sub(r'[^\\w\\s]', '', article)\n",
    "    words = [word.lower() for word in nltk.word_tokenize(article)]\n",
    "    words = [word for word in words if not word in stop_words]\n",
    "    words_lemmatized = []\n",
    "    for word in words:\n",
    "        if word == 'us':\n",
    "            words_lemmatized.append(word)\n",
    "        else:\n",
    "            words_lemmatized.append(WNL.lemmatize(word))\n",
    "    filtered_article = \" \".join([word for word in words_lemmatized])\n",
    "    df.loc[index, 'news'] = filtered_article\n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6335x80967 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1762247 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorization\n",
    "\n",
    "df_input = df['news']\n",
    "df_output = df['label']\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tf_idf_matrix = vectorizer.fit_transform(df_input)\n",
    "tf_idf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our input vectors, the rows of the matrix `tf_idf_matrix`, and our output labels, `df_output`.\n",
    "\n",
    "In order to perform logistic regression, we first need to split the data into test and train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are performing a 75:25 split.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(tf_idf_matrix, df_output, random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can input the training data into a logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "logreg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 0, 1, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.predict(x_test[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring model performance\n",
    "\n",
    "We now have a logistic regression model fit to our training dataset. We are going to use accuracy as our metric to measure the model's performance. To do this we are going to see how the model performs on the test data. The accuracy is calculated as a percentage of the number of correct predictions in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9172979797979798"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = logreg.score(x_test, y_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our accuracy is 91.7%. Not bad!\n",
    "\n",
    "Let's see if changing the ratio of test to train data has any impact on our accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fa101de6750>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sizes = np.arange(0.01,0.5,0.01)\n",
    "accuracys = []\n",
    "for i in test_sizes:\n",
    "    x_train, x_test, y_train, y_test = train_test_split(tf_idf_matrix, df_output, test_size=i, random_state=23)\n",
    "    logreg.fit(x_train, y_train)\n",
    "    accuracy = logreg.score(x_test, y_test)\n",
    "    accuracys.append(accuracy)\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(test_sizes, accuracys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24000000000000002"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sizes[np.argmax(accuracys)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this model gives the greatest accuracy when we use the 75:25 ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "A good way of visualising the performance of our classifier is by using a confusion matrix. This is a matrix where the actual class is compared with the predicted class, so we can see where our model is going wrong most often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(y_test,logreg.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1505,   71],\n",
       "       [ 211, 1318]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that of the $1576$ fake news articles in the test dataset, $1505$ were predicted correctly as fake by the model and $71$ were incorrectly predicted to be real. Of the $1529$ real news articles in the test dataset, $211$ were incorrectly predicted as fake and $1318$ were correctly predicted as real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate of model on fake news articles: 0.9549492385786802\n",
      "Success rate of model on real news articles: 0.8620013080444735\n"
     ]
    }
   ],
   "source": [
    "print('Success rate of model on fake news articles: ' + str(1505/(1505+71)))\n",
    "print('Success rate of model on real news articles: ' + str(1318/(211+1318)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our model is more accurate when classifying fake news compared to real news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation set\n",
    "\n",
    "Our logistic regression model has several different hyperparameters which we can finetune in order to perhaps increase the accuracy of our model. However, if we use the test dataset whilst finetuning these hyperparameters then the test data may become biased as our model will have already seen it. So we leave the test data unseen, only to be used once the model is complete to give a true representation of the model's accuracy. Instead, we split the data into a third part, called the validation set, which we can use to finetune the hyperparameters. We shall use a 60:20:20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% train and 20% test.\n",
    "x_train3, x_test3, y_train3, y_test3 = train_test_split(tf_idf_matrix, df_output, test_size=0.2, random_state=23)\n",
    "\n",
    "# Split train 75:25 as 0.8*0.25=0.2\n",
    "# So now have 60% train, 20% validation and 20% test.\n",
    "x_train3, x_val3, y_train3, y_val3 = train_test_split(x_train3, y_train3, test_size=0.25, random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression model has several different hyperparameters which may affect the accuracy. Different solvers `solver` can affect convergence, regularisation `penalty` can sometimes be helpful and the `C` parameter controls the penality strength, which can also be affective. Now let's try tweaking some of these hyperparameters to see, using the validation dataset, whether we can improve the accuracy of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8958168902920284 Solver: lbfgs\n",
      "Accuracy: 0.8966061562746646 Solver: liblinear\n",
      "Accuracy: 0.8958168902920284 Solver: newton-cg\n",
      "Accuracy: 0.8958168902920284 Solver: sag\n",
      "Accuracy: 0.8966061562746646 Solver: saga\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(solver='lbfgs')\n",
    "logreg.fit(x_train3, y_train3)\n",
    "print('Accuracy: ' + str(logreg.score(x_val3,y_val3)) + ' Solver: lbfgs')\n",
    "logreg = LogisticRegression(solver='liblinear')\n",
    "logreg.fit(x_train3, y_train3)\n",
    "print('Accuracy: ' + str(logreg.score(x_val3,y_val3)) + ' Solver: liblinear')\n",
    "logreg = LogisticRegression(solver='newton-cg')\n",
    "logreg.fit(x_train3, y_train3)\n",
    "print('Accuracy: ' + str(logreg.score(x_val3,y_val3)) + ' Solver: newton-cg')\n",
    "logreg = LogisticRegression(solver='sag')\n",
    "logreg.fit(x_train3, y_train3)\n",
    "print('Accuracy: ' + str(logreg.score(x_val3,y_val3)) + ' Solver: sag')\n",
    "logreg = LogisticRegression(solver='saga')\n",
    "logreg.fit(x_train3, y_train3)\n",
    "print('Accuracy: ' + str(logreg.score(x_val3,y_val3)) + ' Solver: saga')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that the liblinear and saga solvers have a slightly greater accuracy, but there's really not much difference.\n",
    "\n",
    "To test all different combinations of `solver`, `penalty` and `C`, we can use a gridsearch technique.\n",
    "\n",
    "The function `GridSearchCV` has a built in method for splitting the data into training and validation data. In fact, its default is to use a method called 5-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxkirwan/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/maxkirwan/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "solvers = ['lbfgs','liblinear','newton-cg','sag','saga']\n",
    "penalties = ['l1', 'l2', 'elasticnet', 'none']\n",
    "C_values = [100,10,1,0.1,0.01]\n",
    "grid = dict(solver=solvers, penalty=penalties, C=C_values)\n",
    "grid_search = GridSearchCV(logreg, grid, n_jobs=-1, scoring='accuracy', error_score=0)\n",
    "grid_result = grid_search.fit(x_train, y_train)\n",
    "\n",
    "# We may get some warnings as not all combinations work, e.g. not all solvers support all regularisation terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 (0.0) with: {'C': 100, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "0.904953560371517 (0.007221779061681416) with: {'C': 100, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "0.0 (0.0) with: {'C': 100, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "0.0 (0.0) with: {'C': 100, 'penalty': 'l1', 'solver': 'sag'}\n",
      "0.9195046439628483 (0.004506208858686921) with: {'C': 100, 'penalty': 'l1', 'solver': 'saga'}\n",
      "0.9213622291021671 (0.0038777631277955417) with: {'C': 100, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.9213622291021671 (0.0038777631277955417) with: {'C': 100, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.9213622291021671 (0.0038777631277955417) with: {'C': 100, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.9241486068111455 (0.0036055770956495738) with: {'C': 100, 'penalty': 'l2', 'solver': 'sag'}\n",
      "0.9256965944272446 (0.0007026703740774441) with: {'C': 100, 'penalty': 'l2', 'solver': 'saga'}\n",
      "0.0 (0.0) with: {'C': 100, 'penalty': 'elasticnet', 'solver': 'lbfgs'}\n",
      "0.0 (0.0) with: {'C': 100, 'penalty': 'elasticnet', 'solver': 'liblinear'}\n",
      "0.0 (0.0) with: {'C': 100, 'penalty': 'elasticnet', 'solver': 'newton-cg'}\n",
      "0.0 (0.0) with: {'C': 100, 'penalty': 'elasticnet', 'solver': 'sag'}\n",
      "0.0 (0.0) with: {'C': 100, 'penalty': 'elasticnet', 'solver': 'saga'}\n",
      "0.9235294117647059 (0.006066241615727879) with: {'C': 100, 'penalty': 'none', 'solver': 'lbfgs'}\n",
      "0.0 (0.0) with: {'C': 100, 'penalty': 'none', 'solver': 'liblinear'}\n",
      "0.9229102167182662 (0.0013487086581349876) with: {'C': 100, 'penalty': 'none', 'solver': 'newton-cg'}\n",
      "0.9260061919504644 (0.0030041269342833475) with: {'C': 100, 'penalty': 'none', 'solver': 'sag'}\n",
      "0.9253869969040248 (0.0034710573075677005) with: {'C': 100, 'penalty': 'none', 'solver': 'saga'}\n",
      "0.0 (0.0) with: {'C': 10, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "0.9043343653250774 (0.00640176957992008) with: {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "0.0 (0.0) with: {'C': 10, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "0.0 (0.0) with: {'C': 10, 'penalty': 'l1', 'solver': 'sag'}\n",
      "0.908359133126935 (0.004392880303201839) with: {'C': 10, 'penalty': 'l1', 'solver': 'saga'}\n",
      "0.9198142414860682 (0.005780858679281461) with: {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.9198142414860682 (0.005780858679281461) with: {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.9198142414860682 (0.005780858679281461) with: {'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.9198142414860682 (0.005780858679281461) with: {'C': 10, 'penalty': 'l2', 'solver': 'sag'}\n",
      "0.920123839009288 (0.005453718316832553) with: {'C': 10, 'penalty': 'l2', 'solver': 'saga'}\n",
      "0.0 (0.0) with: {'C': 10, 'penalty': 'elasticnet', 'solver': 'lbfgs'}\n",
      "0.0 (0.0) with: {'C': 10, 'penalty': 'elasticnet', 'solver': 'liblinear'}\n",
      "0.0 (0.0) with: {'C': 10, 'penalty': 'elasticnet', 'solver': 'newton-cg'}\n",
      "0.0 (0.0) with: {'C': 10, 'penalty': 'elasticnet', 'solver': 'sag'}\n",
      "0.0 (0.0) with: {'C': 10, 'penalty': 'elasticnet', 'solver': 'saga'}\n",
      "0.9235294117647059 (0.006066241615727879) with: {'C': 10, 'penalty': 'none', 'solver': 'lbfgs'}\n",
      "0.0 (0.0) with: {'C': 10, 'penalty': 'none', 'solver': 'liblinear'}\n",
      "0.9229102167182662 (0.0013487086581349876) with: {'C': 10, 'penalty': 'none', 'solver': 'newton-cg'}\n",
      "0.9247678018575851 (0.0012820704369082673) with: {'C': 10, 'penalty': 'none', 'solver': 'sag'}\n",
      "0.9260061919504644 (0.0017201626746424403) with: {'C': 10, 'penalty': 'none', 'solver': 'saga'}\n",
      "0.0 (0.0) with: {'C': 1, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "0.8535603715170279 (0.010595233237532897) with: {'C': 1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "0.0 (0.0) with: {'C': 1, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "0.0 (0.0) with: {'C': 1, 'penalty': 'l1', 'solver': 'sag'}\n",
      "0.8526315789473684 (0.010422438762452716) with: {'C': 1, 'penalty': 'l1', 'solver': 'saga'}\n",
      "0.8953560371517028 (0.004998918250763998) with: {'C': 1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.8959752321981425 (0.004562178199499983) with: {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.8953560371517028 (0.004998918250763998) with: {'C': 1, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.8953560371517028 (0.004998918250763998) with: {'C': 1, 'penalty': 'l2', 'solver': 'sag'}\n",
      "0.8962848297213623 (0.00433438736424261) with: {'C': 1, 'penalty': 'l2', 'solver': 'saga'}\n",
      "0.0 (0.0) with: {'C': 1, 'penalty': 'elasticnet', 'solver': 'lbfgs'}\n",
      "0.0 (0.0) with: {'C': 1, 'penalty': 'elasticnet', 'solver': 'liblinear'}\n",
      "0.0 (0.0) with: {'C': 1, 'penalty': 'elasticnet', 'solver': 'newton-cg'}\n",
      "0.0 (0.0) with: {'C': 1, 'penalty': 'elasticnet', 'solver': 'sag'}\n",
      "0.0 (0.0) with: {'C': 1, 'penalty': 'elasticnet', 'solver': 'saga'}\n",
      "0.9235294117647059 (0.006066241615727879) with: {'C': 1, 'penalty': 'none', 'solver': 'lbfgs'}\n",
      "0.0 (0.0) with: {'C': 1, 'penalty': 'none', 'solver': 'liblinear'}\n",
      "0.9229102167182662 (0.0013487086581349876) with: {'C': 1, 'penalty': 'none', 'solver': 'newton-cg'}\n",
      "0.9272445820433437 (0.0010981470580277047) with: {'C': 1, 'penalty': 'none', 'solver': 'sag'}\n",
      "0.9250773993808049 (0.0018560769248376354) with: {'C': 1, 'penalty': 'none', 'solver': 'saga'}\n",
      "0.0 (0.0) with: {'C': 0.1, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "0.7371517027863778 (0.0225756671599873) with: {'C': 0.1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "0.0 (0.0) with: {'C': 0.1, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "0.0 (0.0) with: {'C': 0.1, 'penalty': 'l1', 'solver': 'sag'}\n",
      "0.7380804953560371 (0.021154751174606874) with: {'C': 0.1, 'penalty': 'l1', 'solver': 'saga'}\n",
      "0.8442724458204335 (0.012030154250276072) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.8464396284829722 (0.013379517276843195) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.8442724458204335 (0.012030154250276072) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.8442724458204335 (0.012030154250276072) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'sag'}\n",
      "0.8445820433436533 (0.011613875579647536) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'saga'}\n",
      "0.0 (0.0) with: {'C': 0.1, 'penalty': 'elasticnet', 'solver': 'lbfgs'}\n",
      "0.0 (0.0) with: {'C': 0.1, 'penalty': 'elasticnet', 'solver': 'liblinear'}\n",
      "0.0 (0.0) with: {'C': 0.1, 'penalty': 'elasticnet', 'solver': 'newton-cg'}\n",
      "0.0 (0.0) with: {'C': 0.1, 'penalty': 'elasticnet', 'solver': 'sag'}\n",
      "0.0 (0.0) with: {'C': 0.1, 'penalty': 'elasticnet', 'solver': 'saga'}\n",
      "0.9235294117647059 (0.006066241615727879) with: {'C': 0.1, 'penalty': 'none', 'solver': 'lbfgs'}\n",
      "0.0 (0.0) with: {'C': 0.1, 'penalty': 'none', 'solver': 'liblinear'}\n",
      "0.9229102167182662 (0.0013487086581349876) with: {'C': 0.1, 'penalty': 'none', 'solver': 'newton-cg'}\n",
      "0.9256965944272446 (0.004576687303390495) with: {'C': 0.1, 'penalty': 'none', 'solver': 'sag'}\n",
      "0.9269349845201238 (0.004347436303568124) with: {'C': 0.1, 'penalty': 'none', 'solver': 'saga'}\n",
      "0.0 (0.0) with: {'C': 0.01, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "0.491640866873065 (7.317613453197922e-06) with: {'C': 0.01, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "0.0 (0.0) with: {'C': 0.01, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "0.0 (0.0) with: {'C': 0.01, 'penalty': 'l1', 'solver': 'sag'}\n",
      "0.508359133126935 (7.317613453197922e-06) with: {'C': 0.01, 'penalty': 'l1', 'solver': 'saga'}\n",
      "0.8207430340557276 (0.022048195110608473) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.8142414860681114 (0.017185706251824057) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.8207430340557276 (0.022048195110608473) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.820123839009288 (0.02241340969041342) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'sag'}\n",
      "0.8207430340557276 (0.021631917709680898) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'saga'}\n",
      "0.0 (0.0) with: {'C': 0.01, 'penalty': 'elasticnet', 'solver': 'lbfgs'}\n",
      "0.0 (0.0) with: {'C': 0.01, 'penalty': 'elasticnet', 'solver': 'liblinear'}\n",
      "0.0 (0.0) with: {'C': 0.01, 'penalty': 'elasticnet', 'solver': 'newton-cg'}\n",
      "0.0 (0.0) with: {'C': 0.01, 'penalty': 'elasticnet', 'solver': 'sag'}\n",
      "0.0 (0.0) with: {'C': 0.01, 'penalty': 'elasticnet', 'solver': 'saga'}\n",
      "0.9235294117647059 (0.006066241615727879) with: {'C': 0.01, 'penalty': 'none', 'solver': 'lbfgs'}\n",
      "0.0 (0.0) with: {'C': 0.01, 'penalty': 'none', 'solver': 'liblinear'}\n",
      "0.9229102167182662 (0.0013487086581349876) with: {'C': 0.01, 'penalty': 'none', 'solver': 'newton-cg'}\n",
      "0.9263157894736842 (0.0016863033668792122) with: {'C': 0.01, 'penalty': 'none', 'solver': 'sag'}\n",
      "0.9250773993808049 (0.0030330879239245604) with: {'C': 0.01, 'penalty': 'none', 'solver': 'saga'}\n"
     ]
    }
   ],
   "source": [
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"{} ({}) with: {}\".format(mean, stdev, param))\n",
    "    \n",
    "# A score of 0 means that it is not possible for the model to use this combination of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.9272445820433437 using {'C': 1, 'penalty': 'none', 'solver': 'sag'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best: {} using {}\".format(grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore we can conclude that the above combination of hyperparameters is optimal if we want to maximise the accuracy of our logistic regression classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our model on unseen data\n",
    "\n",
    "Now that we have a working model, we would like to use it to classify unseen news articles.\n",
    "However there is one major problem: the way that our model is built is by using the Tf-idf vectoriser, which creates a vector based on the frequency of terms in the entire corpus. So the only way to add articles to our model will be to alter these vectors. This means that we cannot just input new articles into our original model and get a classification. Instead, each time we would like to input a new unknown article, we will have to rebuild the Tf-idf matrix. Luckily this isn't too tricky and actually could be a good thing - it means that our model is always improving with each new article added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a text pre-processor function.\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['the','it','in'])\n",
    "WNL = WordNetLemmatizer()\n",
    "    \n",
    "\n",
    "def article_preprocessor (article):\n",
    "    filtered_article = ''\n",
    "    article = re.sub(r'[^\\w\\s]', '', article)\n",
    "    words = [word.lower() for word in nltk.word_tokenize(article)]\n",
    "    words = [word for word in words if not word in stop_words]\n",
    "    words_lemmatized = []\n",
    "    for word in words:\n",
    "        if word == 'us':\n",
    "            words_lemmatized.append(word)\n",
    "        else:\n",
    "            words_lemmatized.append(WNL.lemmatize(word))\n",
    "    filtered_article = \" \".join([word for word in words_lemmatized])\n",
    "    return filtered_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_classifier (list_of_articles):\n",
    "    \n",
    "    # Pre-process the articles\n",
    "    articles_pp = [article_preprocessor(article) for article in list_of_articles]\n",
    "    \n",
    "    # Append new articles to original list of articles (df_input)\n",
    "    new_input = df_input.append(pd.Series(articles_pp))\n",
    "    \n",
    "    # Vectorisation of new_input\n",
    "    tf_idf_matrix = vectorizer.fit_transform(new_input)\n",
    "    \n",
    "    # Split matrix into original dataset and new data\n",
    "    orig_data_matrix = tf_idf_matrix[:len(df_input)]\n",
    "    new_data_matrix = tf_idf_matrix[len(df_input):]\n",
    "    \n",
    "    # Now perform logistic regression on original dataset\n",
    "    x_train, x_test, y_train, y_test = train_test_split(orig_data_matrix, df_output, random_state=23)\n",
    "    logreg = LogisticRegression(solver='lbfgs')\n",
    "    logreg.fit(x_train, y_train)\n",
    "    accuracy = logreg.score(x_test,y_test)\n",
    "    print('Logistic regression model accuracy: ' + str(accuracy))\n",
    "    # The model can now classify the new data\n",
    "    predictions = logreg.predict(new_data_matrix)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have built a function to detect whether new articles are fake or not, let's put it to the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The top news story on the BBC\n",
    "bbc_news_article = '''The furlough scheme will be extended until the end of September by the chancellor in the Budget later.\n",
    "Rishi Sunak said the scheme - which pays 80% of employees' wages for the hours they cannot work in the pandemic - would help millions through \"the challenging months ahead\".\n",
    "Some 600,000 more self-employed people will also be eligible for government help as access to grants is widened.\n",
    "But Labour said the support schemes should have been extended \"months ago\".\n",
    "Mr Sunak will outline a three-point plan to support people through the coming months, rebuild the economy and \"fix\" the public finances in the wake of the pandemic when he delivers his statement to the Commons at about 12:30 GMT.\n",
    "But he has warned of tough economic times ahead and there are reports that he plans to raise some taxes.'''\n",
    "\n",
    "# Here's a fake news article from the New York Mag\n",
    "fake_article = '''Twelve days out from judgment day in an election in which he continues to trail badly, President Trump continues to hammer home an issue that will surely resonate with that small slice of still-undecided voters: his supposedly unfair treatment at the hands of CBS’s Lesley Stahl. After two days of promising to release unedited footage of an as-yet-unaired 60 Minutes interview, during which he walked out prematurely because he was upset with Stahl’s line of questioning, the president finally followed through on Thursday. Throughout the interview, Stahl presses Trump on issues from health care (the president says he hopes the Supreme Court strikes down Obamacare, a politically toxic position) to his derogatory comments about Anthony Fauci (Trump claims he was misinterpreted) to his false claims that the Obama campaign spied on him. The tone is of an adversarial back-and-forth, well within normal journalistic bounds. Nevertheless, Trump continuously claims that Joe Biden hasn’t been given similar treatment by CBS and cuts the proceedings short.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression model accuracy: 0.9172979797979798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = [bbc_news_article,fake_article]\n",
    "logreg_classifier(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Our classifier has correctly predicted that the BBC news article is real `1` and that the NYmag article is fake `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
