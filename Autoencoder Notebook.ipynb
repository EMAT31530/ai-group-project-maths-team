{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv('news.csv')\n",
    "df = df.drop(df.columns[0],axis=1)\n",
    "df['news'] = df['title'] + ' ' + df['text']\n",
    "convert_to_binary = {'REAL':1,'FAKE':0}\n",
    "df['label'] = df['label'].map(convert_to_binary)\n",
    "df = df.drop([df.columns[0],df.columns[1]],axis=1)\n",
    "df = df.reindex(columns=['news','label'])\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import re\n",
    "\n",
    "# Creating a list of stopwords (common words which are not important to the overall meaning of the article).\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['the','it','in'])\n",
    "\n",
    "# Lemmatization is the process of reducing words to their basic form, e.g. dogs->dog, running->run.\n",
    "WNL = WordNetLemmatizer()\n",
    "\n",
    "# We now iterate through each article in the dataset.\n",
    "for index, row in df.iterrows():\n",
    "    filtered_article = ''\n",
    "    article = row['news']\n",
    "    # Removing all punctuation using RegEx.\n",
    "    article = re.sub(r'[^\\w\\s]', '', article)\n",
    "    # Tokenization: changing the article from one continuous string to a list of tokens (words).\n",
    "    words = [word.lower() for word in nltk.word_tokenize(article)]\n",
    "    # Removing stopwords.\n",
    "    words = [word for word in words if not word in stop_words]\n",
    "    # Lemmatization and concatenating the words back into a single string.\n",
    "    words_lemmatized = []\n",
    "    for word in words:\n",
    "        if word == 'us':\n",
    "            words_lemmatized.append(word)\n",
    "        else:\n",
    "            words_lemmatized.append(WNL.lemmatize(word))\n",
    "    filtered_article = \" \".join([word for word in words_lemmatized])\n",
    "    \n",
    "    # Replace the article with the new filtered version.\n",
    "    df.loc[index, 'news'] = filtered_article\n",
    "    \n",
    "df_input = df['news']\n",
    "df_output = df['label']\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tf_idf_matrix = vectorizer.fit_transform(df_input)\n",
    "\n",
    "x = tf_idf_matrix.toarray()\n",
    "labels = df_output.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (2.4.3)\n",
      "Requirement already satisfied: pyyaml in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from keras) (5.1.2)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from keras) (1.19.5)\n",
      "Requirement already satisfied: h5py in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from keras) (1.3.1)\n",
      "Requirement already satisfied: six in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from h5py->keras) (1.15.0)\n",
      "Requirement already satisfied: tensorflow in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (2.4.1)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (3.15.5)\n",
      "Requirement already satisfied: wheel~=0.35 in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.36.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: tensorboard~=2.4 in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (2.4.1)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: six~=1.15.0 in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.32.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.11.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.19.5)\n",
      "Requirement already satisfied: h5py~=2.10.0 in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (0.4.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (41.4.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (1.27.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (0.23)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2019.9.11)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: more-itertools in /Users/gus/opt/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (7.2.0)\n"
     ]
    }
   ],
   "source": [
    "#RUN\n",
    "\n",
    "# installing and importanting neccessary modules (mainly Keras)\n",
    "\n",
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "\n",
    "# setting random seed (for consistent results & choosing operating system)\n",
    "\n",
    "from numpy.random import seed\n",
    "from tensorflow.random import set_seed\n",
    "seed(0)\n",
    "set_seed(0)\n",
    "import os\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example point's binary outcome: 1\n",
      "same example point's one hot encoded vector: [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# RUN\n",
    "\n",
    "# 'hot encoding' the data (go from binary 0 or 1 to [1,0] and [0,1] respectively)\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "print(\"example point's binary outcome:\", labels[100])\n",
    "labels = to_categorical(labels)\n",
    "print(\"same example point's one hot encoded vector:\", labels[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "\n",
    "# splitting the data into a training data set (60%), a validation data set (20%) and a test data set (20%) \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, labels, test_size=0.2)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "\n",
    "# scaling the training and validation data features. Removes mean and makes the datas variance = 1. \n",
    "# learns the correct transformation on the training data and then applies to the test data.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "\n",
    "x_val = scaler.transform(x_val)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "\n",
    "# importing things we need to build a neural network\n",
    "from keras.models import Sequential # going to use a sequential neural network\n",
    "from keras.layers import Dense # type of layer\n",
    "from keras.optimizers import SGD # optimisation technique (stochastic gradient descent) used to update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from keras.datasets import mnist\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "\n",
    "def create_model(x_train, y_train, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Model providing function:\n",
    "    \n",
    "    Create Keras model with double curly brackets dropped-in as needed.\n",
    "    Return value has to be a valid python dictionary with two customary keys:\n",
    "        - loss: Specify a numeric evaluation metric to be minimized\n",
    "        - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
    "    The last one is optional, though recommended, namely:\n",
    "        - model: specify the model just created so that we can later use it again.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense({{choice([32, 64, 128,256])}}, input_dim=80967, activation='relu'))\n",
    "    model.add(Dense({{choice([32, 64, 128,256])}}, activation='relu'))\n",
    "    model.add(Dense({{choice([16, 32, 64,128])}}, activation='relu'))#,name='encoded')\n",
    "    model.add(Dense({{choice([8, 16, 32,64])}}, activation='relu'))\n",
    "    model.add(Dense({{choice([16, 32, 8])}}, activation='relu'))\n",
    "    model.add(Dense({{choice([32, 64, 16,8])}}, activation='relu'))\n",
    "    if {{choice(['five', 'seven'])}} == 'five':\n",
    "        model.add(Dense(80967))\n",
    "    else:\n",
    "        model.add(Dense({{choice([32,64])}}, activation='relu'))\n",
    "        model.add(Dense({{choice([32,64])}}, activation='relu'))\n",
    "        model.add(Dense(80967))\n",
    "        # We can also choose between complete sets of layers\n",
    "\n",
    "    model.compile(loss='mse', metrics=['mse'],\n",
    "                  optimizer={{choice(['adam', 'sgd'])}}(learning_rate={{uniform(0, 1)}}))\n",
    "\n",
    "    result = model.fit(x_train, y_train,\n",
    "              batch_size={{choice([3000, 4500, 4751])}},\n",
    "              epochs=50,\n",
    "              validation_split=0.2)\n",
    "    #get the highest validation accuracy of the training epochs\n",
    "    validation_mse = np.amax(result.history['val_mse']) \n",
    "    print('Best validation acc of epoch:', validation_acc)\n",
    "    return {'loss': validation_mse, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import nltk\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from nltk.tokenize import word_tokenize\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from nltk.corpus import stopwords\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from nltk.stem import WordNetLemmatizer\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import re\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import Model\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from numpy.random import seed\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.random import set_seed\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import to_categorical\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import SGD\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.datasets import mnist\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import np_utils\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Dense': hp.choice('Dense', [32, 64, 128,256]),\n",
      "        'Dense_1': hp.choice('Dense_1', [32, 64, 128,256]),\n",
      "        'Dense_2': hp.choice('Dense_2', [16, 32, 64,128]),\n",
      "        'Dense_3': hp.choice('Dense_3', [8, 16, 32,64]),\n",
      "        'Dense_4': hp.choice('Dense_4', [16, 32, 8]),\n",
      "        'Dense_5': hp.choice('Dense_5', [32, 64, 16,8]),\n",
      "        'Dense_6': hp.choice('Dense_6', ['five', 'seven']),\n",
      "        'Dense_7': hp.choice('Dense_7', [32,64]),\n",
      "        'Dense_8': hp.choice('Dense_8', [32,64]),\n",
      "        'optimizer': hp.choice('optimizer', ['adam', 'sgd']),\n",
      "        'learning_rate': hp.uniform('learning_rate', 0, 1),\n",
      "        'batch_size': hp.choice('batch_size', [3000, 4500, 4751]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "1: \n",
      "2: \n",
      "3: \n",
      "4: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     \"\"\"\n",
      "   4:     Model providing function:\n",
      "   5:     \n",
      "   6:     Create Keras model with double curly brackets dropped-in as needed.\n",
      "   7:     Return value has to be a valid python dictionary with two customary keys:\n",
      "   8:         - loss: Specify a numeric evaluation metric to be minimized\n",
      "   9:         - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
      "  10:     The last one is optional, though recommended, namely:\n",
      "  11:         - model: specify the model just created so that we can later use it again.\n",
      "  12:     \"\"\"\n",
      "  13:     model = Sequential()\n",
      "  14:     model.add(Dense(space['Dense'], input_dim=80967, activation='relu'))\n",
      "  15:     model.add(Dense(space['Dense_1'], activation='relu'))\n",
      "  16:     model.add(Dense(space['Dense_2'], activation='relu'))#,name='encoded')\n",
      "  17:     model.add(Dense(space['Dense_3'], activation='relu'))\n",
      "  18:     model.add(Dense(space['Dense_4'], activation='relu'))\n",
      "  19:     model.add(Dense(space['Dense_5'], activation='relu'))\n",
      "  20:     if space['Dense_6'] == 'five':\n",
      "  21:         model.add(Dense(80967))\n",
      "  22:     else:\n",
      "  23:         model.add(Dense(space['Dense_7'], activation='relu'))\n",
      "  24:         model.add(Dense(space['Dense_8'], activation='relu'))\n",
      "  25:         model.add(Dense(80967))\n",
      "  26:         # We can also choose between complete sets of layers\n",
      "  27: \n",
      "  28:     model.compile(loss='mse', metrics=['mse'],\n",
      "  29:                   optimizer=space['optimizer'](learning_rate=space['learning_rate']))\n",
      "  30: \n",
      "  31:     result = model.fit(x_train, y_train,\n",
      "  32:               batch_size=space['batch_size'],\n",
      "  33:               epochs=50,\n",
      "  34:               validation_split=0.2)\n",
      "  35:     #get the highest validation accuracy of the training epochs\n",
      "  36:     validation_mse = np.amax(result.history['val_mse']) \n",
      "  37:     print('Best validation acc of epoch:', validation_acc)\n",
      "  38:     return {'loss': validation_mse, 'status': STATUS_OK, 'model': model}\n",
      "  39: \n",
      "\r",
      "  0%|          | 0/5 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "job exception: 'str' object is not callable\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?trial/s, best loss=?]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d2ac4690352d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                           \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                           \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                                      notebook_name ='Autoencoder Notebook')\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/hyperas/optim.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(model, data, algo, max_evals, trials, functions, rseed, notebook_name, verbose, eval_space, return_space, keep_temp)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                      \u001b[0mnotebook_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnotebook_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                                      \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                                      keep_temp=keep_temp)\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/hyperas/optim.py\u001b[0m in \u001b[0;36mbase_minimizer\u001b[0;34m(model, data, functions, algo, max_evals, trials, rseed, full_model_string, notebook_name, verbose, stack, keep_temp)\u001b[0m\n\u001b[1;32m    137\u001b[0m              \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m              \u001b[0mrstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m              return_argmin=True),\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0mget_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    520\u001b[0m             \u001b[0mshow_progressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             \u001b[0mearly_stop_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stop_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m             \u001b[0mtrials_save_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrials_save_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m         )\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0mshow_progressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0mearly_stop_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stop_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m             \u001b[0mtrials_save_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrials_save_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m         )\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0;31m# next line is where the fmin is actually executed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m                     \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"job exception: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    905\u001b[0m                 \u001b[0mprint_node_on_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrec_eval_print_node_on_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m             )\n\u001b[0;32m--> 907\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Maths /3rd Year/Intro to AI/Group Project/GitHub/ai-group-project-maths-team/temp_model.py\u001b[0m in \u001b[0;36mkeras_fmin_fnct\u001b[0;34m(space)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "def data2():\n",
    "    return x_train, y_train, x_val, y_val\n",
    "\n",
    "best_run, best_model = optim.minimize(model=create_model,\n",
    "                                          data=data2,\n",
    "                                          algo=tpe.suggest,\n",
    "                                          max_evals=5,\n",
    "                                          trials=Trials(),\n",
    "                                     notebook_name ='Autoencoder Notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
